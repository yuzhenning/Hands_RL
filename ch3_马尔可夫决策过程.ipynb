{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d415d84a-2f65-443d-977a-b830505a54d9",
   "metadata": {},
   "source": [
    "马尔可夫决策过程（Markov decision process，MDP）是强化学习的重要概念。\n",
    "要学好强化学习，我们首先要掌握马尔可夫决策过程的基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234539f0-1cea-4502-9fe0-973fc0acc123",
   "metadata": {},
   "source": [
    "3.2.1 随机过程 \n",
    "随机过程（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，\n",
    "而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838fd6a-ad71-43ae-a668-ee28bc629e39",
   "metadata": {},
   "source": [
    "在随机过程中，随机现象在某时刻t的取值是一个向量随机变量，用St表示，所有可能的状态组成状态集合S。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2b1c6-e640-4999-a290-1a7e53d7fb4b",
   "metadata": {},
   "source": [
    "当且仅当某时刻的状态只取决于上一时刻的状态时，\n",
    "一个随机过程被称为具有马尔可夫性质（Markov property），\n",
    "用公式表示为:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af6d7d-2f74-48f2-84fd-da126a513276",
   "metadata": {},
   "source": [
    "$P(S_{t+1}|S_t) = P(S_{t+1}|S_1,...,S_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e86700-49a7-435d-898b-2c9990c17d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据本序列计算得到回报为：-2.5。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# 定义状态转移概率矩阵P\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\n",
    "gamma = 0.5  # 定义折扣因子\n",
    "\n",
    "\n",
    "# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报\n",
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))):\n",
    "        G = gamma * G + rewards[chain[i] - 1]\n",
    "    return G\n",
    "\n",
    "\n",
    "# 一个状态序列,s1-s2-s3-s6\n",
    "chain = [1, 2, 3, 6]\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(\"根据本序列计算得到回报为：%s。\" % G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8d1bc-7a8d-4836-aae5-21d26107abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "在马尔可夫奖励过程中，\n",
    "一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）\n",
    "被称为这个状态的价值（value）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad280438-c1d8-4373-9e10-0de3512799f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRP中每个状态价值分别为\n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def compute(P, rewards, gamma, states_num):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
    "                   rewards)\n",
    "    return value\n",
    "\n",
    "\n",
    "V = compute(P, rewards, gamma, 6)\n",
    "print(\"MRP中每个状态价值分别为\\n\", V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155db21f-a5cd-4d92-baa5-c8bf5f47f9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
