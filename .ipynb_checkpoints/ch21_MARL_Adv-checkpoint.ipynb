{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7ad73f-8c42-4b17-9ca9-5fa2e2b7030b",
   "metadata": {},
   "source": [
    "# 第 21 章 多智能体强化学习进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038734a-f879-4228-a0d6-07276d5765b3",
   "metadata": {},
   "source": [
    "## 21.1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5e617-dfd2-445e-a525-2189bf414ef1",
   "metadata": {},
   "source": [
    "第 20 章中已经初步介绍了多智能体强化学习研究的问题和最基本的求解范式。本章来介绍一种比较经典且效果不错的进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）。所谓中心化训练去中心化执行是指在训练的时候使用一些单个智能体看不到的全局信息而以达到更好的训练效果，而在执行时不使用这些信息，每个智能体完全根据自己的策略直接动作以达到去中心化执行的效果。中心化训练去中心化执行的算法能够在训练时有效地利用全局信息以达到更好且更稳定的训练效果，同时在进行策略模型推断时可以仅利用局部信息，使得算法具有一定的扩展性。CTDE 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b529d-6312-4607-b82c-fbad3c46801a",
   "metadata": {},
   "source": [
    "CTDE 算法主要分为两种：一种是基于值函数的方法，例如 VDN，QMIX 算法等；另一种是基于 Actor-Critic 的方法，例如 MADDPG 和 COMA 等。本章将重点介绍 MADDPG 算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcd4d7-f579-412a-a4e2-cb9e9ab5593e",
   "metadata": {},
   "source": [
    "## 21.2 MADDPG 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2586b4-0c24-4c02-bc9e-771341c628d4",
   "metadata": {},
   "source": [
    "多智能体 DDPG（muli-agent DDPG，MADDPG）算法从字面意思上来看就是对于每个智能体实现一个 DDPG 的算法。所有智能体共享一个中心化的 Critic 网络，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导，而执行时每个智能体的 Actor 网络则是完全独立做出行动，即去中心化地执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16daa88-8aeb-4e15-899c-f2b755513410",
   "metadata": {},
   "source": [
    "## 21.3 MADDPG 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d972c6-ffab-499a-b2ec-830b0e109fd9",
   "metadata": {},
   "source": [
    "下面我们来看看如何实现 MADDPG 算法，首先是导入一些需要用到的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7ed016-51a7-4547-ab2b-b26af32d1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404edb0-0275-4cf3-9735-707d37d1cf00",
   "metadata": {},
   "source": [
    "我们要使用的环境为多智能体粒子环境（multiagent particles environment，MPE），它是一些面向多智能体交互的环境的集合，在这个环境中，粒子智能体可以移动、通信、“看”到其他智能体，也可以和固定位置的地标交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25af8b5-6dc0-4f4c-a769-a38ba5894916",
   "metadata": {},
   "source": [
    "接下来安装环境，由于 MPE 的官方仓库的代码已经不再维护了，而其依赖于 gym 的旧版本，因此我们需要重新安装 gym 库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823a41e9-15e1-4a7b-9906-3a9d01577b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.sjtu.edu.cn/pypi/web/simple\n",
      "Obtaining file:///home/yubuntust/DLearning_git/Hands_RLearning/multiagent-particle-envs\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym in /home/yubuntust/miniconda3/lib/python3.12/site-packages (from multiagent==0.0.1) (0.25.2)\n",
      "Collecting numpy-stl (from multiagent==0.0.1)\n",
      "  Downloading https://mirror.sjtu.edu.cn/pypi-packages/ea/2c/e17b8814050427929077639d35a42187a006922600d4840475bdc5f64ebb/numpy_stl-3.2.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/yubuntust/miniconda3/lib/python3.12/site-packages (from gym->multiagent==0.0.1) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/yubuntust/miniconda3/lib/python3.12/site-packages (from gym->multiagent==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /home/yubuntust/miniconda3/lib/python3.12/site-packages (from gym->multiagent==0.0.1) (0.0.8)\n",
      "Collecting python-utils>=3.4.5 (from numpy-stl->multiagent==0.0.1)\n",
      "  Downloading https://mirror.sjtu.edu.cn/pypi-packages/d4/69/31c82567719b34d8f6b41077732589104883771d182a9f4ff3e71430999a/python_utils-3.9.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: typing_extensions>3.10.0.2 in /home/yubuntust/miniconda3/lib/python3.12/site-packages (from python-utils>=3.4.5->numpy-stl->multiagent==0.0.1) (4.11.0)\n",
      "Installing collected packages: python-utils, numpy-stl, multiagent\n",
      "\u001b[33m  DEPRECATION: Legacy editable install of multiagent==0.0.1 from file:///home/yubuntust/DLearning_git/Hands_RLearning/multiagent-particle-envs (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py develop for multiagent\n",
      "Successfully installed multiagent-0.0.1 numpy-stl-3.2.0 python-utils-3.9.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiagent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAgentEnv\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiagent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscenarios\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mscenarios\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_env\u001b[39m(scenario_name):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# 从环境文件脚本中创建环境\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     scenario \u001b[38;5;241m=\u001b[39m scenarios\u001b[38;5;241m.\u001b[39mload(scenario_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mScenario()\n",
      "File \u001b[0;32m~/DLearning_git/Hands_RLearning/multiagent-particle-envs/multiagent/scenarios/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mosp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imp'"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/boyu-ai/multiagent-particle-envs.git --quiet\n",
    "!pip install -e multiagent-particle-envs\n",
    "import sys\n",
    "sys.path.append(\"multiagent-particle-envs\")\n",
    "# 由于multiagent-pariticle-env底层的实现有一些版本问题,因此gym需要改为可用的版本\n",
    "!pip install --upgrade gym==0.10.5 -q\n",
    "import gym\n",
    "from multiagent.environment import MultiAgentEnv\n",
    "import multiagent.scenarios as scenarios\n",
    "\n",
    "\n",
    "def make_env(scenario_name):\n",
    "    # 从环境文件脚本中创建环境\n",
    "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
    "    world = scenario.make_world()\n",
    "    env = MultiAgentEnv(world, scenario.reset_world, scenario.reward,\n",
    "                        scenario.observation)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920f1c0-c644-4335-b198-cdbc6216d055",
   "metadata": {},
   "source": [
    "本章选择 MPE 中的simple_adversary环境作为代码实践的示例，如图 21-2 所示。\n",
    "该环境中有 1 个红色的对抗智能体（adversary）、N个蓝色的正常智能体，以及N个地点（一般N=2），这N个地点中有一个是目标地点（绿色）。这N个正常智能体知道哪一个是目标地点，但对抗智能体不知道。正常智能体是合作关系：它们其中任意一个距离目标地点足够近，则每个正常智能体都能获得相同的奖励。对抗智能体如果距离目标地点足够近，也能获得奖励，但它需要猜哪一个才是目标地点。因此，正常智能体需要进行合作，分散到不同的坐标点，以此欺骗对抗智能体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d07a3-12b4-41a7-827e-a7dc2a0a5f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a041a-ddb3-4f72-9c07-29b8ae13a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    ''' 生成最优动作的独热（one-hot）形式 '''\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作,转换成独热形式\n",
    "    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[\n",
    "        np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "    ]],\n",
    "                                       requires_grad=False).to(logits.device)\n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\"从Gumbel(0,1)分布中采样\"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(),\n",
    "                                requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(\n",
    "        logits.device)\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\"从Gumbel-Softmax分布中采样,并进行离散化\"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y\n",
    "    # 返回一个y_hard的独热量,但是它的梯度是y,我们既能够得到一个与环境交互的离散动作,又可以\n",
    "    # 正确地反传梯度\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390c077-5c9c-4e40-8d06-2ff7119fe430",
   "metadata": {},
   "source": [
    "接着实现我们的单智能体 DDPG。其中包含 Actor 网络与 Critic 网络，以及计算动作的函数，这在第 13 章中的已经介绍过，此处不再赘述。但这里没有更新网络参数的函数，其将会在 MADDPG 类中被实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5286f83-c9fe-4668-a300-e6f7d7b0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim,\n",
    "                 actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim,\n",
    "                                       hidden_dim).to(device)\n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1,\n",
    "                                        hidden_dim).to(device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) +\n",
    "                                    param.data * tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399cb7a-506f-47d4-9223-dc2286bec0a4",
   "metadata": {},
   "source": [
    "接下来正式实现一个 MADDPG 类，该类对于每个智能体都会维护一个 DDPG 算法。它们的策略更新和价值函数更新使用的是 21.2 节中关于和的公式给出的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04809a-4b85-4f37-b27d-d61f760a5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.critic_criterion = torch.nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    def take_action(self, states, explore):\n",
    "        states = [\n",
    "            torch.tensor([states[i]], dtype=torch.float, device=self.device)\n",
    "            for i in range(len(env.agents))\n",
    "        ]\n",
    "        return [\n",
    "            agent.take_action(state, explore)\n",
    "            for agent, state in zip(self.agents, states)\n",
    "        ]\n",
    "\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            onehot_from_logits(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = rew[i_agent].view(\n",
    "            -1, 1) + self.gamma * cur_agent.target_critic(\n",
    "                target_critic_input) * (1 - done[i_agent].view(-1, 1))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        critic_loss = self.critic_criterion(critic_value,\n",
    "                                            target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_act_vf_in)\n",
    "            else:\n",
    "                all_actor_acs.append(onehot_from_logits(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873947f5-9641-464d-aefd-259e89f1530c",
   "metadata": {},
   "source": [
    "现在我们来定义一些超参数，创建环境、智能体以及经验回放池并准备训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026f48b-6001-435a-8cb5-d1279d85dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "episode_length = 25  # 每条序列的最大长度\n",
    "buffer_size = 100000\n",
    "hidden_dim = 64\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "batch_size = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update_interval = 100\n",
    "minimal_size = 4000\n",
    "\n",
    "env_id = \"simple_adversary\"\n",
    "env = make_env(env_id)\n",
    "replay_buffer = rl_utils.ReplayBuffer(buffer_size)\n",
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for action_space in env.action_space:\n",
    "    action_dims.append(action_space.n)\n",
    "for state_space in env.observation_space:\n",
    "    state_dims.append(state_space.shape[0])\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d70d9-9fd5-486f-ab6a-34ef5ae69aeb",
   "metadata": {},
   "source": [
    "接下来实现以下评估策略的方法，之后就可以开始训练了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab8bd6-4faa-4f37-b0c2-f60c64313013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_id, maddpg, n_episode=10, episode_length=25):\n",
    "    # 对学习的策略进行评估,此时不会进行探索\n",
    "    env = make_env(env_id)\n",
    "    returns = np.zeros(len(env.agents))\n",
    "    for _ in range(n_episode):\n",
    "        obs = env.reset()\n",
    "        for t_i in range(episode_length):\n",
    "            actions = maddpg.take_action(obs, explore=False)\n",
    "            obs, rew, done, info = env.step(actions)\n",
    "            rew = np.array(rew)\n",
    "            returns += rew / n_episode\n",
    "    return returns.tolist()\n",
    "\n",
    "\n",
    "return_list = []  # 记录每一轮的回报（return）\n",
    "total_step = 0\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    # ep_returns = np.zeros(len(env.agents))\n",
    "    for e_i in range(episode_length):\n",
    "        actions = maddpg.take_action(state, explore=True)\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "        replay_buffer.add(state, actions, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size(\n",
    "        ) >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample(batch_size)\n",
    "\n",
    "            def stack_array(x):\n",
    "                rearranged = [[sub_x[i] for sub_x in x]\n",
    "                              for i in range(len(x[0]))]\n",
    "                return [\n",
    "                    torch.FloatTensor(np.vstack(aa)).to(device)\n",
    "                    for aa in rearranged\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "            for a_i in range(len(env.agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        ep_returns = evaluate(env_id, maddpg, n_episode=100)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7de705-8930-4584-bf1e-bb4d7dd90703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72af3d3a-dbce-43fa-8ac7-2c0f5b138508",
   "metadata": {},
   "source": [
    "训练结束，我们来看看训练效果如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3fa11-a3bb-40cd-ba0c-0d3c169eba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_array = np.array(return_list)\n",
    "for i, agent_name in enumerate([\"adversary_0\", \"agent_0\", \"agent_1\"]):\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        np.arange(return_array.shape[0]) * 100,\n",
    "        rl_utils.moving_average(return_array[:, i], 9))\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Returns\")\n",
    "    plt.title(f\"{agent_name} by MADDPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef42fd-a42d-4426-a136-55e2894e9adb",
   "metadata": {},
   "source": [
    "可以看到，正常智能体agent_0和agent_1的回报结果完全一致，这是因为它们的奖励函数完全一样。正常智能体最终保持了正向的回报，说明它们通过合作成功地占领了两个不同的地点，进而让对抗智能体无法知道哪个地点是目标地点。另外，我们也可以发现 MADDPG 的收敛速度和稳定性都比较不错。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c397489-3d2d-4755-88b6-d5fb8aec45c5",
   "metadata": {},
   "source": [
    "## 21.4 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387e641-c23c-4ba7-a204-22273c9c5c87",
   "metadata": {},
   "source": [
    "本章讲解了多智能体强化学习 CTDE 范式下的经典算法 MADDPG，MADDPG 后续也衍生了不少多智能体强化学习算法。因此，理解 MADDPG 对深入探究多智能体算法非常关键，有兴趣的读者可阅读 MADDPG 原论文加深理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5af4c-93b7-4210-80e2-3bb9c2ca4ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
