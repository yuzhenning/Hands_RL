{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef96e17-e25f-4128-b685-e3302b7a1b7d",
   "metadata": {},
   "source": [
    "# 第 20 章 多智能体强化学习入门"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6426fd4-38cb-4181-9d0e-c8c786749c8e",
   "metadata": {},
   "source": [
    "## 20.1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc229a3-d47e-4aff-853f-40b4aebfe1a0",
   "metadata": {},
   "source": [
    "本书之前介绍的算法都是单智能体强化学习算法，其基本假设是动态环境是稳态的（stationary），即状态转移概率和奖励函数不变，并依此来设计相应的算法。而如果环境中还有其他智能体做交互和学习，那么任务则上升为多智能体强化学习（multi-agent reinforcement learning，MARL），如图 20-1 所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03318b1d-c73b-4c09-962e-2fc46dc0a9cd",
   "metadata": {},
   "source": [
    "多智能体的情形相比于单智能体更加复杂，因为每个智能体在和环境交互的同时也在和其他智能体进行直接或者间接的交互。因此，多智能体强化学习要比单智能体更困难，其难点主要体现在以下几点："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d61c8-30e5-4e25-b85b-28768a5fe426",
   "metadata": {},
   "source": [
    "* 由于多个智能体在环境中进行实时动态交互，并且每个智能体在不断学习并更新自身策略，因此在每个智能体的视角下，环境是非稳态的（non-stationary），即对于一个智能体而言，即使在相同的状态下采取相同的动作，得到的状态转移和奖励信号的分布可能在不断改变；\n",
    "* 多个智能体的训练可能是多目标的，不同智能体需要最大化自己的利益；\n",
    "* 训练评估的复杂度会增加，可能需要大规模分布式训练来提高效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b592718-2324-46be-9861-70212c934b80",
   "metadata": {},
   "source": [
    "## 20.2 问题建模"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2f12d-d17c-42c7-b86c-ca8c78a53005",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "337586f9-01a2-4001-af3c-031eacbf8d88",
   "metadata": {},
   "source": [
    "## 20.3 多智能体强化学习的基本求解范式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72bb34-19f8-462c-b321-8a13a3e9e613",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c5c0c6-7061-45c7-a4c5-4e347b2ee519",
   "metadata": {},
   "source": [
    "## 20.4 IPPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab82578-9a2e-46b8-ac0e-4b24f0ed82ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a22391f2-d6df-4684-9e2e-059a8e14da9b",
   "metadata": {},
   "source": [
    "## 20.5 IPPO 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca8f7f-aaf2-422f-b9b4-fcb6da7b40a3",
   "metadata": {},
   "source": [
    "下面介绍一下要使用的多智能体环境：ma_gym库中的 Combat 环境。\n",
    "Combat 是一个在二维的格子世界上进行的两个队伍的对战模拟游戏，每个智能体的动作集合为：向四周移动 1 格，攻击周围$3\\times3$格范围内其他敌对智能体，或者不采取任何行动。起初每个智能体有 3 点生命值，如果智能体在敌人的攻击范围内被攻击到了，则会扣 1 生命值，生命值掉为 0 则死亡，最后存活的队伍获胜。每个智能体的攻击有一轮的冷却时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c89ed-fe9e-4673-92e6-053fc2dd3c99",
   "metadata": {},
   "source": [
    "在游戏中，我们能够控制一个队伍的所有智能体与另一个队伍的智能体对战。另一个队伍的智能体使用固定的算法：攻击在范围内最近的敌人，如果攻击范围内没有敌人，则向敌人靠近。图 20-2 是一个简单的 Combat 环境示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654f0cf-2988-4a31-b335-9e16403ba142",
   "metadata": {},
   "source": [
    "首先仍然导入一些需要用到的包，然后从 GitHub 中克隆ma-gym仓库到本地，并且导入其中的 Combat 环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b5fe193-f905-4f14-8188-6d2dd35f2ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EnvSpec' object has no attribute '_kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ma-gym\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mma_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Combat\n",
      "File \u001b[0;32m~/DLearning_git/Hands_RLearning/ma-gym/ma_gym/__init__.py:15\u001b[0m\n\u001b[1;32m     10\u001b[0m env_specs \u001b[38;5;241m=\u001b[39m [env_spec \u001b[38;5;28;01mfor\u001b[39;00m env_spec \u001b[38;5;129;01min\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgym.envs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m env_spec\u001b[38;5;241m.\u001b[39mentry_point]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m env_specs:\n\u001b[1;32m     12\u001b[0m     register(\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mma_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m spec\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     14\u001b[0m         entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mma_gym.envs.openai:MultiAgentWrapper\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 15\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: spec\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mspec\u001b[38;5;241m.\u001b[39m_kwargs}\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# add new environments : iterate over full observability\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, observability \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EnvSpec' object has no attribute '_kwargs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import rl_utils\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ! git clone https://github.com/boyu-ai/ma-gym.git\n",
    "import sys\n",
    "sys.path.append(\"./ma-gym\")\n",
    "from ma_gym.envs.combat.combat import Combat\n",
    "# WARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\n",
    "# pip install gym==0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e92016-14e7-4f8a-a1d5-d16f7d25c522",
   "metadata": {},
   "source": [
    "接下来的代码块与 12.4 节介绍过的 PPO 代码实践基本一致，不再赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912ef9b-43b4-4fbe-ba60-dc169df3d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    ''' PPO算法,采用截断方式 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 lmbda, eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n",
    "                                                                       dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,\n",
    "                                               td_delta.cpu()).to(self.device)\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,\n",
    "                                                            actions)).detach()\n",
    "\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps,\n",
    "                            1 + self.eps) * advantage  # 截断\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "        critic_loss = torch.mean(\n",
    "            F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42549f62-3dbe-4a66-b42f-aff2041cf334",
   "metadata": {},
   "source": [
    "现在进入 IPPO 代码实践的最主要部分。值得注意的是，在训练时使用了参数共享（parameter sharing）的技巧，即对于所有智能体使用同一套策略参数，这样做的好处是能够使得模型训练数据更多，同时训练更稳定。能够这样做的前提是，两个智能体是同质的（homogeneous），即它们的状态空间和动作空间是完全一致的，并且它们的优化目标也完全一致。感兴趣的读者也可以自行实现非参数共享版本的 IPPO，此时每个智能体就是一个独立的 PPO 的实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e718e-7761-412a-8951-1a7bf372ce0c",
   "metadata": {},
   "source": [
    "和之前的一些实验不同，这里不再展示智能体获得的回报，而是将 IPPO 训练的两个智能体团队的胜率作为主要的实验结果。接下来就可以开始训练 IPPO 了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc89747e-9c74-4493-bf6a-687c6689ffe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Combat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m grid_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m env \u001b[38;5;241m=\u001b[39m Combat(grid_shape\u001b[38;5;241m=\u001b[39mgrid_size, n_agents\u001b[38;5;241m=\u001b[39mteam_size, n_opponents\u001b[38;5;241m=\u001b[39mteam_size)\n\u001b[1;32m     15\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Combat' is not defined"
     ]
    }
   ],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "num_episodes = 100000\n",
    "hidden_dim = 64\n",
    "gamma = 0.99\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "team_size = 2\n",
    "grid_size = (15, 15)\n",
    "#创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env = Combat(grid_shape=grid_size, n_agents=team_size, n_opponents=team_size)\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "action_dim = env.action_space[0].n\n",
    "#两个智能体共享同一个策略\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps,\n",
    "            gamma, device)\n",
    "\n",
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict_1 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            transition_dict_2 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            s = env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                a_1 = agent.take_action(s[0])\n",
    "                a_2 = agent.take_action(s[1])\n",
    "                next_s, r, done, info = env.step([a_1, a_2])\n",
    "                transition_dict_1['states'].append(s[0])\n",
    "                transition_dict_1['actions'].append(a_1)\n",
    "                transition_dict_1['next_states'].append(next_s[0])\n",
    "                transition_dict_1['rewards'].append(\n",
    "                    r[0] + 100 if info['win'] else r[0] - 0.1)\n",
    "                transition_dict_1['dones'].append(False)\n",
    "                transition_dict_2['states'].append(s[1])\n",
    "                transition_dict_2['actions'].append(a_2)\n",
    "                transition_dict_2['next_states'].append(next_s[1])\n",
    "                transition_dict_2['rewards'].append(\n",
    "                    r[1] + 100 if info['win'] else r[1] - 0.1)\n",
    "                transition_dict_2['dones'].append(False)\n",
    "                s = next_s\n",
    "                terminal = all(done)\n",
    "            win_list.append(1 if info[\"win\"] else 0)\n",
    "            agent.update(transition_dict_1)\n",
    "            agent.update(transition_dict_2)\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(win_list[-100:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e135d-1621-497f-8661-bc6efd9bd27c",
   "metadata": {},
   "source": [
    "plot figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2cbd7-5eae-444b-9c08-b84c00f5d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_array = np.array(win_list)\n",
    "#每100条轨迹取一次平均\n",
    "win_array = np.mean(win_array.reshape(-1, 100), axis=1)\n",
    "\n",
    "episodes_list = np.arange(win_array.shape[0]) * 100\n",
    "plt.plot(episodes_list, win_array)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Win rate')\n",
    "plt.title('IPPO on Combat')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558763b2-9d6b-4b3b-8c74-a7bf0cc01808",
   "metadata": {},
   "source": [
    "可以看出，当智能体数量较少的时候，IPPO 这种完全去中心化学习在一定程度上能够取得好的效果，但是最终达到的胜率也比较有限。这可能是因为多个智能体之间无法有效地通过合作来共同完成目标。同时，好奇的读者也可以尝试增加智能体的数量，比较一下训练结果。当数量增加到 5 时，这种完全去中心化学习的训练效果就不是很好了。这时候可能就需要引入更多的算法来考虑多个智能体之间的交互行为，或者使用中心化训练去中心化执行（centralized training with decentralized execution，CTDE）的范式来进行多智能体训练，该方法将在第 21 章中详细介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88162e38-ad2a-482a-baf4-2318b7450a43",
   "metadata": {},
   "source": [
    "## 20.6 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec95c6-280e-4cdb-96a3-cc62e706409c",
   "metadata": {},
   "source": [
    "本章介绍了多智能体强化学习的概念和两类基本的解决范式，并针对其中的完全去中心化方法进行了详细的介绍，讲解了一个具体的算法 IPPO，即用 PPO 算法为各个智能体训练各自的策略。在 Combat 环境中，我们共享了两个智能体之间的策略，以达到更好的效果。但这仅限于多个智能体同质的情况，若它们的状态空间或动作空间不一致，那便无法进行策略共享。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80658d97-0752-471d-8ea5-34ea78a8a830",
   "metadata": {},
   "source": [
    "## 20.7 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a130c3-77ad-442e-9529-b9eb96662ed0",
   "metadata": {},
   "source": [
    "[1] HERNANDEZ L P, BILAL K, TAYLOR M E. A survey and critique of multiagent deep reinforcement learning[J]. Autonomous Agents and Multi-Agent Systems, 2019, 33(6): 750-797.\n",
    "\n",
    "[2] TAMPUU A, MATIISEN T, KODELJA D, et al. Multiagent cooperation and competition with deep reinforcement learning [J]. PloS One, 2017; 12(4): e0172395.\n",
    "\n",
    "[3] TAN M. Multi-agent reinforcement learning: independent vs. cooperative agents [C]// International conference on machine learning, 1993: 330-337.\n",
    "\n",
    "[4] Combat 环境（参见 GitHub 网站中的 koulanurag/ma-gym 项目）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843c9b0-ee33-4073-8267-52a9333d574f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d011a60-1f49-45b2-8663-a4ed1ba1c104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
